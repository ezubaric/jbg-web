\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{mfirstuc}
\usepackage{colortbl}
\usepackage{epstopdf}
\usepackage{mdwlist}
\usepackage{url}

\newcommand{\abr}[1]{\textsc{#1}}
\newcommand{\newcite}[2]{\capitalisewords{#1} et al.~\cite{#1-#2}}



\newcommand{\student}[1]{\vspace{.5cm}\fbox{\parbox{0.95\linewidth}{{\small
        #1}}}\vspace{.5cm}}
\providecommand{\blue}[1]{{\color{blue}{#1}}}
\providecommand{\red}[1]{{\color{red}{#1}}}
\providecommand{\green}[1]{{\color{green}{#1}}}

\begin{document}

 \title{Research Statement: Evaluating and Enabling Human--AI Collaboration}

 \author{Jordan Boyd-Graber, University of Maryland}
%\institute{University of Colorado, Boulder CO 80309, USA}


\date{Updated April 2025}

\maketitle

\ifumd
\vspace{.2cm}
  \parbox{\linewidth}{I have read the following and certify that this
  personal statement is a current and accurate statement of my
  professional record to the best of my
  knowledge \flushright  \includegraphics[width=.2\linewidth]{resume_src/signature} \\
\flushright  (\today{})}
\vspace{.5cm}
\fi


Artificial intelligence\footnote{I take a broad interpretation of
\abr{ai}; some of my examples might be better characterized machine
learning.  But rather than distracting boundary policing, I will embrace
the general term but will be specific in describing particular tools/models.}
(\abr{ai}) is ubiquitous: detecting spam e-mails, flagging fraudulent
purchases, and providing the next movie in a Netflix binge.
%
But they do not exist in a vacuum: as
Shneiderman~\cite{shneiderman-21} argues, \abr{ai} must exist
alongside humans.
%
My goal is to create metrics to measure whether \abr{ai} methods make
sense to users, helping users craft examples to advance \abr{ai}, and
applying \abr{ai} to illuminate complex social
science applications.

\section{Evaluating Interpretability}

My journey with evaluating interpretability began fifteen years ago
with topic models.
%
Topic models are sold as a tool for understanding large data
collections: lawyers scouring Nordstream e-mails for a smoking gun,
journalists making sense of Wikileaks, or humanists characterizing the
oeuvre of Lope de Vega.
%
But topic models' proponents never asked what those lawyers,
journalists, or humanists needed.
%
Instead, they optimized \emph{held-out likelihood}.

When my colleagues
and I developed the \emph{interpretability} measure to assess whether topic
models' users understood their outputs, interpretability and
held-out likelihood were negatively correlated~\cite{chang-09b}!
%
The topic modeling community (including me) had fetishized complexity
at the expense of usability\dots and topic modeling is not alone.

\begin{center}
\includegraphics[width=.5\linewidth]{images/prec_ll_4}
\end{center}

Since this humbling discovery, I've built topic models that are a
collaboration between humans and computers.  The computer starts by
proposing an organization of the data.  The user separates confusing
clusters or joins similar clusters together~\cite{hu-14:itm}, an
improvement over the ``take it or leave it'' philosophy of most
machine learning algorithms.
%
Focusing on collaboration with as many people as possible also requires algorithms that are low
latency~\cite{lund-17} and
multi-lingual~\cite{Yuan-18}.

After we proposed our ``reading tea leaves'' evaluation, it's
heartening that \newcite{lau}{14} and their ``machine reading tea
leaves'' (which correlate with our human measures) became a standard
topic model evaluation: in a survey of forty recent topic modeling
papers, {\bf all but four} use a form of their coherence evaluation.
%
However, as we argue in \newcite{hoyle}{21}, you cannot just use this
evaluation forever and forget about humans.
%
In that same survey, {\bf none} of those papers do a human evaluation.
%
As topic models evolve (e.g., incorporating
neural components), you need to validate that these automatic metrics
still correlate with whether it is useful for a human--computer
collaboration~\cite{Li:Mao:Stephens:Goel:Walpole:Fung:Dima:Boyd-Graber-2024}.

\section{Teaming as an Evaluation}

Within the \abr{hci} community, we have argued for the foundations of
what should go into human--computer collaborations: computers that incorporate
users' suggestions~\cite{kumar-19}; explanations with
accountability~\cite{smith-20}; and stable
explanations~\cite{smith-20:adherence}.

In addition to these human-centered understanding of users' needs and
desires, we've developed machine learning approaches to measure how
well users complete a task.
%
For example, for a question answering task, we measured how much the
accuracy of the human--computer \emph{team} increases with different
explanations and found that explanations help all users but that
novices are easily overwhelmed~\cite{feng-19}.
%
In follow-on work, we learned how to explicitly optimize explanations
for individual users~\cite{feng-22} or to help users in negotiations~\cite{Gu:Wongkamjan:Kummerfeld:Peskoff:May:Boyd-Graber-2025}.


\section{Connecting with Social Science: Pedagogy, Framing, and Deception}

The reverse of cooperation is human competition; it also has much to
teach computers.
%
I've increasingly looked at language-based games whose clear goals and
intrinsic fun speed research progress.
%
For example, in the board game \emph{Diplomacy}, users
chat with each other while marshaling armies for world conquest. Alliances are
fluid: friends are betrayed and enemies embraced as the game develops.
%
However,
users' conversations let us predict when friendships break.

Thus, we argued that Diplomacy would be an exciting testbed for
natural language processing, and our 2015 paper is---to the best of
our knowledge---the first \abr{nlp} research on Diplomacy.
%
Before a betrayal, betrayers write ostensibly friendly messages and become more polite, stop talking about the future, and limit how \emph{much} they write~\cite{niculae-15}.
%
In follow-on work, we developed a dataset that predicts both when users
lie to each other and when recipients of lies detect
deception~\cite{Peskov-20}.
%
Diplomacy may be a nerdy game, but it is a fruitful testbed to teach
computers to understand messy, emotional human interactions.


Recently, the use of \abr{nlp} methods in the game of Diplomacy has
been the subject of highly-publicized papers by DeepMind in Nature
Communications~\cite{kramar-22} and Meta in Science~\cite{bakhtin-22}.
%
The DeepMind paper built a game theoretic understanding of when
betrayal should happen, building on our descriptive investigation of
deception in human games.
%
The Meta paper, like our 2020 paper, used a classifier to detect
deceptive statements but went far beyond our work by building an AI to play Diplomacy.
%
In our most recent work, we showed that even though Meta's \textit{tour de force} agent can consistently beat humans, it is viewed as less trustworthy than human players: humans lie to AI more than other humans and humans view it as lying more (even though it lies less).
%
Moreover, it was less persuasive---i.e., able to change people's minds---than humans despite its strategic supremacy~\cite{Wongkamjan-2024}.

%A game with higher stakes is politics. However, just like Diplomacy,
%the words that people use reveal their underlying goals; computational
%methods can help expose the ``moves'' political players can use. With
%collaborators in political science, we've built models that: show when
%politicians in debates strategically change the topic to influence
%others~\cite{nguyen-12,Nguyen-14b}; frame topics to reflect political
%leanings~\cite{nguyen-13:shlda}; use subtle linguistic phrasing to
%express their political leaning~\cite{iyyer-14a}; or create political
%subgroups with larger political
%movements~\cite{Nguyen:Boyd-Graber:Resnik:Miler-2015}.

Persuasion online is not just arguments and interpersonal relations; it's often about \emph{mis}information.
%
Thus, we have focused on developing fact checking: datasets
for general knowledge fact checking~\cite{eisenschlos-21} and climate
change fact checking~\cite{Diggelmann-20}.
%
However, not all misinformation is written so I'm also exploring multimodal misleading information with journalism professor Naeemul Hassan~\cite{Sung:Hassan:Boyd-Graber-2023}.

\section{Human-in-the-Loop Adversarial Examples}

One of the most fun aspects of my research has been building
trivia-playing robots~\cite{boyd-graber-12,iyyer-14b,iyyer-15}; beyond research papers, our system has faced off against former Jeopardy
champions in front of hundreds high school
students\footnote{\url{https://www.youtube.com/watch?v=LqsUaprYMOw}}
and against researchers at NeurIPS 2015 (which won the best
demonstration award).
%
But after defeating some of the smartest trivia players, did I
actually believe that our system was better at question answering?
%
No!

Adversarial examples first came out of the vision community: add a
small epsilon to an example and suddenly a object detector calls a
turtle a gun~\cite{athalye-18}.\footnote{Point of personal pride: I
mentored Kevin on another research project~\cite{he-16}, but
I myself had nothing to do with this later adversarial work.}
%
While others have attempted to create adversarial examples for
language using paraphrasing, it's hard to know if the changes are
perceptually negligible (``who wrote the invisible man''---a question with the answer H.G. Wells---is
fundamentally different from ``who wrote the man you can't see''---an ill-formed questions---as is
``who wrote the book invisible man''---a question with the answer Ralph Ellison) and
it's hard to ``add epsilon'' to a discrete word.

Consistent with the theme of my research, my \abr{nsf career} grant
added a \emph{human in the loop} to generate novel adversarial
language examples that can provide new training examples to make
\abr{ai} more robust and to expose what \abr{ai} cannot (yet) do.
%
With Eric Wallace, an undergraduate student, we built a system that
could help an expert trivia question writer to stump a computer: as
the author writes the question, it shows the author what the system is
thinking~\cite{wallace-18}.
%
And it worked, even generalizing across models~\cite{wallace-19} (many
examples written with an \abr{ir} model still stump a neural model).
%
After we introduced human-in-the-loop adversarial example generation,
Meta/Facebook adopted this framework with gusto~\cite{bartolo-20} in
their Dynabench framework, the Dynamic Adversarial Data Collection
workshop and call for proposals (which I'm grateful funded our
continuing research in this area).

Adversarial examples have become known as ``jailbreaks'' in the computer security community, and we have also built a dataset with human-in-the-loop adversarial examples that revealed new attach methods such as context filling~\cite{Schulhoff:Pinto:Khan:Bouchard:Si:Boyd-Graber:Anati:Tagliabue:Kost:Carnahan-2023} (this paper won the outstanding Theme Paper award at EMNLP 2023).
%
However, because models are improving so quickly, it is difficult to know how ``adversarial'' an example is and whether what is adversarial on Monday will remain adversarial on Thursday.
%
Thus, we developed a new metric (AdvScore) based on item response theory to empirically measure how difficult examples are for both humans and humans~\cite{Sung:Gor:Fleisig:Mondal:Boyd-Graber-2025}: adversarial examples are naturally those with the biggest gap between human and computer difficulty.

\section{But wait, there's more!}

Many of our best-cited papers are ``traditional'' papers that do
better on some task:
%
\begin{itemize*}
\item We developed deep averaging networks~\cite[\abr{dan}]{iyyer-15}, a simple model still used in the 
transformer age~\cite{ye-22}.
\item In question answering, we proposed new evaluation mechanisms
  for knowing if an answer is correct~\cite{si-21} and improved
  information retrieval to answer complicated
  questions~\cite{elgohary-19,shi-20,zhao-21}.

\item We also introduced reinforcement learning to \emph{simultaneous
machine interpretation}~\cite{Grissom:He:Boyd-Graber:Morgan-2014}, a
  language-based task that requires significant human intuition,
  insight, and---for those who want to become
  interpreters---training.\footnote{This framework---using
  reinforcement learning to capture human strategies---was featured in
  Liang Huang's \abr{acl} keynote.} We learned tricks from
  professional human interpreters---passivizing sentences and guessing
  the verb---to translate sentences sooner~\cite{He-15}, letting
  speakers and algorithms cooperate together and enabling more natural
  cross-cultural communication.  We also use reinforcement
  learning to learn machine translation feedback from noisy
  supervision such as star ratings on a webpage~\cite{nguyen-17}.
  
  \item We deployed a chatbot to assist new and expectant mothers~\cite{Srikanth:Sarkar:Y.:M.:C.:Rudinger:Boyd-Graber-2024}, focusing on techniques to detect questions with incorrect pragmatic assumptions (i.e., a mother asking ``when can I resume breastfeeding after a cold'' suggests the mother believes they should \emph{stop} breastfeeding during a cold, when that's the only way infants can get the appropriate antibodies).  Many modern language models just accept those false assumptions rather than correct them.  We are evaluating the efficacy of this support in a randomized control trial with partners in public health~\cite{Y.:Amara:Ximena:Michelle:Xiaohe:Srikanth:Sourabh:Abby:Ann:Pragat:Xin:Boyd-Graber:M.:C.-2023}.
\end{itemize*}

This work doesn't \emph{yet} fit nicely into the human--computer
collaboration narrative, but these more complex tasks are part of my
broader vision for where my research will go: state-of-the-art models
built to support human decisions, not replace them.  And that requires
the low-latency models built to react to input ``like a human''
described above.

\section{Future Work}

\textbf{Multimodal.}
%
Our adversarial work with language models is nearing an end: it's getting harder and harder to stump them.
%
However, models still struggle with low frequency words when answering spoken questions, understanding abstract representational art, or mapping a schematic to the real world.
%
These leaps of understanding are still a core component of human intelligence, so we will continue to craft adversarial domains but in new modalities to better understand AI limits and highlight human skills that have yet to be mastered.

\textbf{Backpropagating Cooperation.}
%
While we have built metrics that reveal when AIs are effectively cooperating with humans (i.e., raising their skill at a task), most AI models are trained with preference data.
%
My student Nishant Balepur has built a sequence of work showing that what users like isn't always what helps them: for example, students don't always prefer the study aids that best improves retention~\cite{Balepur:Shu:Hoyle:Robey:Feng:Goldfarb-Tarrant:Boyd-Graber-2024}.
%
The next step is to actually backpropagate that signal into the internals of a model to see what it looks like when the first priority of a model training is to cooperate with users.
%
However, because distinct individuals need different support, this also requires greater personalization.


\textbf{Data Diversity.}
%
Existing datasets are not diverse and do
not reflect the kinds of interactions people from diverse backgrounds
have with \abr{ai} systems.
%
In question answering, Google's Natural Questions, SQuAD, and other
datasets contain entities that are overwhelmingly male and either
American or British~\cite{gor-21}.
%
In newly funded \abr{nsf} research, we're working with surfacing questions that
reflect a specific \emph{cultural} context: detecting when questions
are answered differently in Ghana than in the \abr{us} or topics that
only people in Bhutan care about (and would be neglected by
\abr{us}-centric datasets).

\textbf{Proxy Models.}
%
While eventually we will need to connect to expensive, difficult to
compute neural models for our Human--AI interactions, many user updates can be approximated by fast
spectral or probabilistic algorithms.
%
We will design fast, browser-based Javascript approximations of these
complicated neural models, to allow users to quickly interact with the
models, get a result, and continue before reconciling the solution later.
%
% However, because these approximations are only approximations, we need
% to reconcile the approximate solution with the ``gold standard'' model
% on a \abr{gpu}-powered server.

At a high level, the goal of my research is to build the future I want for my children: an ecosystem not where AI is replacing humans but where it is put in a position to effectively augment their abilities, no matter their skill level or background.


%% \vspace{12cm}

%%  \parbox{\linewidth}{I certify that this
%%   statement is a current and accurate statement of my
%%   professional record to the best of my
%%   knowledge \flushright  \includegraphics[width=.2\linewidth]{resume_src/signature} \\
%% \flushright  (\today{})}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

