\documentclass[11pt, amssymb, a4paper, one column]{article}
\usepackage{times, amsmath, amssymb, cancel, changepage, url, gensymb, graphicx, mathrsfs, amsthm, lipsum, fancyhdr}
\usepackage[margin=.5in]{geometry}
\usepackage{multicol}


\usepackage[
  backend=bibtex,
  style=numeric,
  sorting=none,
  maxbibnames=99,
  url=false,
  doi=false,
  isbn=false,
  eprint=false
  ]{biblatex}

  \renewcommand*{\bibfont}{\scriptsize}
  \setlength\bibitemsep{0.5\itemsep}

\addbibresource{resume_src/journal-abbrv.bib}
\addbibresource{resume_src/jbg}

\newcommand{\ihat}{\mathbf {\hat \imath}}
\newcommand{\jhat}{\mathbf {\hat \jmath}}



%you only really need a documentclass/ packages to make a document, the following is just nice formatting. You can easily remove it or look up something else
%that you like better

\pagestyle{fancy}
\lhead{Jordan Boyd-Graber, University of Maryland}
\rhead{\large Human-Centered Deep Learning \normalsize}
%note how I use "\#" to denote "#." This is because # is reserved (for something). Similarly, % is reserved for comments, so to use it you need \%.
%other such things include { }.

\fancypagestyle{plain}{}

%end of header, beginning of document

\begin{document}
My research develops deep learning systems that can be \emph{reliably evaluated} and
\emph{meaningfully interact with humans}.
As \textsc{ai} systems increasingly integrate into scientific, social, and organizational
decision-making, raw predictive accuracy is an inadequate standalone metric.
%
Instead, effective systems
must express uncertainty, incorporate human feedback, and operate as agents within
dynamic human environments.

Across my work, I pursue a measurement-driven approach to AI: evaluation,
uncertainty, and interaction are core modeling problems rather than
post-hoc analyses.  While much of my work uses language and multimodal models as
testbeds, the underlying goal is to develop general principles for trustworthy,
interactive, and agentic deep learning systems.

\subsection*{Uncertainty, Evaluation, and Trustworthy AI}

A central theme of my research is that standard evaluation metrics often fail to
reflect whether AI systems are helpful to humans.
Early in my career, I demonstrated that commonly used likelihood-based metrics
could be negatively correlated with human judgments of model interpretability,
motivating a shift toward \emph{human-grounded
  evaluation}~\cite{chang-09b} (rather than perplexity-based) and introducing human-in-the-loop
adversarial examples~\cite{wallace-19}.

Building on this foundation, my recent work focuses on calibration,
hallucination, and robustness in modern language and multimodal models.
I have developed empirical frameworks to compare how adversarial
datasets that claim to be adversarial actually are~\cite{Sung:Gor:Fleisig:Mondal:Boyd-Graber-2025} and to compare model uncertainty directly
against human uncertainty
\cite[CalSore]{Sung:Fleisig:Hope:Upadhyay:Boyd-Graber-2025}, revealing
systematic overconfidence on wrong answers and underconfidence on
correct answers even in
state-of-the-art systems.

I have extended these ideas to multimodal models, where hallucinations are
particularly difficult to detect, designing synthetic and task-based benchmarks
that enable controlled, reproducible analysis~\cite{Wu:Guan:Li:Huang:Liu:Wang:Xian:Shrivastava:Huang:Boyd-Graber:Zhou:Manocha-2024,
Li:Wu:Shi:Qin:Du:Zhou:Manocha:Boyd-Graber-2025}.

\subsection*{Agentic and Interactive Deep Learning Systems}

Many real-world AI systems must act as \emph{agents}: they interact with users over
time, adapt to feedback, and influence human beliefs and decisions.
Long before the recent resurgence of interest in agentic AI, my work focused on
\emph{interactive models} where human input can take computer
overviews of large document collections, get feedback from users, and
then update models' representation of datasets to better reflect users'
needs~\cite[Interactive Topic Models]{hu-14:itm}.

%Si:Goyal:Wu:Zhao:Feng:III:Boyd-Graber-2024

That work led to \emph{ALTO}~\cite{poursabzi-16}, which cast topic modeling as an
active learning problem, and
\emph{BASS}~\cite{Li:Calvo-Bartolome:Hoyle:Xu:Stephens:Fung:Dima:Boyd-Graber-2025},
which integrated \textsc{llm}-guided sensemaking into a process that
resembles grounded theory: an LLM and the user work together to sort
documents into groups that share the same user-interpretable label.
%
When it is obvious which label should be applied to a document, it is
applied automatically; but when it is less clear, the user needs to
either review a suggestion from an LLM or use their own judgement.
%
At a more theoretical level, this requires understanding the world
knowledge of large language models: active learning should direct
attention to the most surprising parts of the latent space~\cite{Yuan:Lin:Boyd-Graber-2020}
%
Together, these systems established human--computer collaboration
methodologies and evaluations that check whether the \emph{process}
answers important questions from datasets.

More recently, I have studied agentic behavior in strategic and social environments.
Using language-based games such as \textit{Diplomacy} as testbeds,
which have been championing for over a
decade~\cite{niculae-15}.
%
We showed that agents optimized solely for
game-theoretic objectives (i.e., Meta's \textit{tour de force} Cicero)
fail to capture essential social and linguistic nuance~\cite{Wongkamjan:Gu:Wang:Hermjakob:May:Stewart:Kummerfeld:Peskoff:Boyd-Graber-2024}.
We developed models and evaluations that incorporate social signaling
and deception, demonstrating that strategically dominant agents can
nevertheless be perceived as less trustworthy and less persuasive than
humans.
%
However, using the reward of well-trained RL agents can more
effectively detect deception using counterfactual reasoning~\cite{Wongkamjan-2024}.

Across these projects, I emphasize \emph{interactive evaluation}:
human-in-the-loop, adversarial, and continual settings that reveal how models adapt
under non-stationary feedback.
This line of work connects agentic AI, continual learning, and the algorithmic
foundations of human--AI interaction.

\subsection*{Personalization and Individual Knowledge Acquisition}

Generic AI systems often fail to account for individual goals, preferences, and
knowledge states.
My research treats personalization as a modeling and evaluation
problem rather than a surface-level interface feature.

I have developed methods for persona induction and preference modeling that enable
language models to adapt responses via abductive reasoning
\cite{Balepur:Padmakumar:Yang:Feng:Rudinger:Boyd-Graber-2025}.
Complementary work
demonstrates that optimizing for stated user preferences can be misaligned with
what actually improves learning or task performance, exposing risks of sycophancy
in preference-tuned models
\cite{Balepur:Shu:Sung:Goldfarb-Tarrant:Feng:Yang:Rudinger:Boyd-Graber-2025}.

In parallel, I have built systems that explicitly measure individual learning
outcomes, including spaced-repetition platforms and mnemonic scaffolding tools
\cite{Balepur:Shu:Hoyle:Robey:Feng:Goldfarb-Tarrant:Boyd-Graber-2024,
Shu:Balepur:Feng:Boyd-Graber-2024}.
These systems connect personalization to quantifiable knowledge acquisition,
providing principled ways to study human--AI complementarity rather than replacement.

\subsection*{Interdisciplinary Impact and Responsible AI}

My research is inherently interdisciplinary, with collaborations spanning political
science (negotiation and framing), journalism (misinformation), ethics (values
inference), public health (randomized controlled trials of AI assistance), and the
life sciences
\cite{Diggelmann-20,Eisenschlos-21,Sung:Hassan:Boyd-Graber-2023,
Srikanth:Sarkar:Y.:M.:C.:Rudinger:Boyd-Graber-2024}.
Across domains, a unifying theme is responsible deployment:
communicating uncertainty, surfacing limitations, and evaluating real-world impact.

This work aligns naturally with initiatives focused on responsible and socially
embedded AI, grounding technical advances in empirical validation and human outcomes.

\subsection*{Future Directions}

\textbf{Multimodal.}
%
Our adversarial work with language models is nearing an end: it's getting harder and harder to stump them.
%
However, models still struggle with low-frequency words in speech, understanding abstract representational art, or mapping schematics to the real world
\cite{Wu:Guan:Li:Huang:Liu:Wang:Xian:Shrivastava:Huang:Boyd-Graber:Zhou:Manocha-2024}.
%
These forms of abstraction remain central to human intelligence, motivating new adversarial benchmarks in underexplored modalities.


\textbf{Backpropagating Cooperation.}
%
While we have built metrics that reveal when AIs effectively cooperate with humans,
most models are trained on preference data.
%
My student Nishant Balepur has shown that what users prefer is not always what helps them,
for example in learning and retention settings
\cite{Balepur:Shu:Hoyle:Robey:Feng:Goldfarb-Tarrant:Boyd-Graber-2024}.
%
The next step is to backpropagate cooperative signals directly into model internals,
while accounting for individual differences through personalization.


\textbf{Data Diversity.}
%
Existing datasets underrepresent the cultural and social diversity of real users.
Major QA benchmarks overwhelmingly center American and British entities
\cite{gor-21}.
%
In newly funded \textsc{nsf} research, we surface culturally grounded questions,
including contexts where answers differ across regions such as Ghana, Bhutan,
and the \textsc{us}.


\textbf{Proxy Models.}
%
To support interactive human--AI systems, we will develop fast spectral and probabilistic
approximations of expensive neural models.
These browser-based proxy models enable rapid interaction and feedback,
with later reconciliation against full \textsc{gpu}-scale models.


\begin{multicols}{2}

\printbibliography

\end{multicols}


\end{document}
