\documentclass[11pt, amssymb, a4paper, one column]{article}
\usepackage{times, amsmath, amssymb, cancel, changepage, url, gensymb, graphicx, mathrsfs, amsthm, lipsum, fancyhdr}
\usepackage[margin=.75in]{geometry}

\newcommand{\ihat}{\mathbf {\hat \imath}}
\newcommand{\jhat}{\mathbf {\hat \jmath}}



%you only really need a documentclass/ packages to make a document, the following is just nice formatting. You can easily remove it or look up something else
%that you like better

\pagestyle{fancy}
\lhead{Jordan Boyd-Graber, University of Maryland}
\rhead{\large Natural Language Processing Shouldn't be a Black Box \normalsize}
%note how I use "\#" to denote "#." This is because # is reserved (for something). Similarly, % is reserved for comments, so to use it you need \%.
%other such things include { }.

\fancypagestyle{plain}{}

%end of header, beginning of document

\begin{document}

My research develops deep learning systems that can be \emph{reliably evaluated} and
\emph{meaningfully interact with humans}.
As \textsc{ai} systems increasingly intertwine in scientific, social, and organizational
decision-making, raw predictive accuracy is an inadequate standalone metric.
%
Instead, effective systems
must express uncertainty, incorporate human feedback, and operate as agents within
dynamic human environments.

Across my work, I pursue a measurement-driven approach to AI: evaluation,
uncertainty, and interaction are treated as core modeling problems rather than
post-hoc analyses.  While much of my work uses language and multimodal models as
testbeds, the underlying goal is to develop general principles for trustworthy,
interactive, and agentic deep learning systems.

\vspace{0.3em}
\subsection*{Uncertainty, Evaluation, and Trustworthy AI}

A central theme of my research is that standard evaluation metrics often fail to
reflect whether AI systems are helpful to humans.
Early in my career, I demonstrated that commonly used likelihood-based metrics
could be negatively correlated with human judgments of model interpretability,
motivating a shift toward \emph{human-grounded evaluation}.

Building on this foundation, my recent work focuses on calibration,
hallucination, and robustness in modern language and multimodal models.
I have developed empirical frameworks to compare model uncertainty directly
against human uncertainty (CalSore), revealing systematic overconfidence even in
state-of-the-art systems. 

I have extended these ideas to multimodal models, where hallucinations are
particularly difficult to detect, designing synthetic and task-based benchmarks
that enable controlled, reproducible analysis.

\vspace{0.3em}
\subsection*{Agentic and Interactive Deep Learning Systems}

Many real-world AI systems must act as \emph{agents}: they interact with users over
time, adapt to feedback, and influence human beliefs and decisions.
Long before the recent resurgence of interest in agentic AI, my work focused on
\emph{interactive learning systems} where human input directly shapes the model and its behavior.

I was among the early developers of \emph{Interactive Topic Modeling}, which enabled
users to guide latent representations during learning, reframing modeling as a
collaborative process.  This work led to \emph{ALTO}, which cast topic modeling as an
active learning problem, and \emph{BASS}, which integrated \textsc{llm}-guided sensemaking into a process that resembles grounded theory.
%
Together, these systems established a human--computer collaboration methodologies and evaluations that check whether the \emph{process} answers important questions from datasets.

More recently, I have studied agentic behavior in strategic and social environments.
Using language-based games such as \textit{Diplomacy} as testbeds, my collaborators and I showed
that agents optimized solely for game-theoretic objectives fail to capture essential
social and linguistic nuance.  We developed models and evaluations that incorporate
social signaling and deception, demonstrating that strategically dominant agents can
nevertheless be perceived as less trustworthy and less persuasive than humans.

Across these projects, I emphasize \emph{interactive evaluation}:
human-in-the-loop, adversarial, and continual settings that reveal how models adapt
under non-stationary feedback.  This line of work connects agentic AI, continual
learning, and the algorithmic foundations of human--AI interaction.

\vspace{0.3em}
\subsection*{Personalization and Individual Knowledge Acquisition}

Generic AI systems often fail to account for individual goals, preferences, and
knowledge states.  My research treats personalization as a modeling and evaluation
problem rather than a surface-level interface feature.

I have developed methods for persona induction and preference modeling that enable
language models to adapt responses via abductive reasoning.  Complementary work
demonstrates that optimizing for stated user preferences can be misaligned with
what actually improves learning or task performance, exposing risks of sycophancy
in preference-tuned models.

In parallel, I have built systems that explicitly measure individual learning
outcomes, including spaced-repetition platforms and mnemonic scaffolding tools.
These systems connect personalization to quantifiable knowledge acquisition,
providing principled ways to study human--AI complementarity rather than replacement.

\vspace{0.3em}
\subsection*{Interdisciplinary Impact and Responsible AI}

My research is inherently interdisciplinary, with collaborations spanning political
science (negotiation and framing), journalism (misinformation), ethics (values
inference), public health (randomized controlled trials of AI assistance), and the
life sciences.  Across domains, a unifying theme is responsible deployment:
communicating uncertainty, surfacing limitations, and evaluating real-world impact.

This work aligns naturally with initiatives focused on responsible and socially
embedded AI, grounding technical advances in empirical validation and human outcomes.

\vspace{0.3em}
\subsection*{Future Directions}


\textbf{Multimodal.}
%
Our adversarial work with language models is nearing an end: it's getting harder and harder to stump them.
%
However, models still struggle with low frequency words when answering spoken questions, understanding abstract representational art, or mapping a schematic to the real world.
%
These leaps of understanding are still a core component of human intelligence, so we will continue to craft adversarial domains but in new modalities to better understand AI limits and highlight human skills that have yet to be mastered.

\textbf{Backpropagating Cooperation.}
%
While we have built metrics that reveal when AIs are effectively cooperating with humans (i.e., raising their skill at a task), most AI models are trained with preference data.
%
My student Nishant Balepur has built a sequence of work showing that what users like isn't always what helps them: for example, students don't always prefer the study aids that best improves retention~\cite{Balepur:Shu:Hoyle:Robey:Feng:Goldfarb-Tarrant:Boyd-Graber-2024}.
%
The next step is to actually backpropagate that signal into the internals of a model to see what it looks like when the first priority of a model training is to cooperate with users.
%
However, because distinct individuals need different support, this also requires greater personalization.


\textbf{Data Diversity.}
%
Existing datasets are not diverse and do
not reflect the kinds of interactions people from diverse backgrounds
have with \textsc{ai} systems.
%
In question answering, Google's Natural Questions, SQuAD, and other
datasets contain entities that are overwhelmingly male and either
American or British~\cite{gor-21}.
%
In newly funded \textsc{nsf} research, we're working with surfacing questions that
reflect a specific \emph{cultural} context: detecting when questions
are answered differently in Ghana than in the \textsc{us} or topics that
only people in Bhutan care about (and would be neglected by
\textsc{us}-centric datasets).

\textbf{Proxy Models.}
%
While eventually we will need to connect to expensive, difficult to
compute neural models for our Human--AI interactions, many user updates can be approximated by fast
spectral or probabilistic algorithms.
%
We will design fast, browser-based Javascript approximations of these
complicated neural models, to allow users to quickly interact with the
models, get a result, and continue before reconciling the solution later.
%
% However, because these approximations are only approximations, we need
% to reconcile the approximate solution with the ``gold standard'' model
% on a \textsc{gpu}-powered server.

\end{document}
