~~ Bibtex | inproceedings
~~ Title | Reading Tea Leaves: How Humans Interpret Topic Models
~~ Author | Jonathan Chang and Jordan Boyd-Graber and Chong Wang and Sean Gerrish and David M. Blei
~~ Booktitle | Neural Information Processing Systems
~~ Year | 2009
~~ Location | Vancouver, BC
~~ Category | Topic Models
~~ Category | Empirical Human Data Collection
~~ Category | Interpretability
~~ Link | Data*downloads/rtl_data.tar.gz
~~ Link | Presentation*docs/nips2009-rtl-pres.pdf
~~ Link | Video*http://videolectures.net/nips09_boyd_graber_rtl/
~~ Url | docs/nips2009-rtl.pdf
~~ Venue | Refereed Conference
~~ Note | Jonathan Chang and I shared a NIPS student award honorable mention for this paper (5 out of 1105)
~~ Embed | <iframe width="280" height="150" src="https://www.youtube.com/embed/ZQLiDh1NJK4" frameborder="0" allowfullscreen></iframe>
~~ Public | Topic models are a tool that historians and social sciences use to explore large text corpora.  But how do you know if you have a good topic model?  Before this paper, the consensus was to use held-out likelihood to evaluate if you had a good model.  This paper argues that this does not fit how people actually use topic models and proposes new human-centered metrics for evaluating topic models.  This method inspired a rethinking of model evaluation and showed that the complexity of a model does not always correspond to what a user might want.
~~ Acceptance | 24
~~ NumPages | 9
