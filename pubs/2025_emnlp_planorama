~~ Bibtex | inproceedings
~~ Title | A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users
~~ Author | Nishant Balepur and Matthew Shu and Yoo Yeon Sung and Seraphina Goldfarb-Tarrant and Shi Feng and Fumeng Yang and Rachel Rudinger and Jordan Boyd-Graber
~~ Booktitle | Empirical Methods in Natural Language Processing
~~ Location | Suzhou and China
~~ Year | 2025
~~ Category | Question Answering
~~ Category | Large Language Models (or and more correctly, <A HREF="https://www.youtube.com/watch?v=u0DgoRVLTE8">Muppet Models</A>)
~~ Category | Empirical Human Data Collection
~~ Venue | Refereed Conference
~~ Project | Personalization*../projects/IIS-2403436.html
~~ Public | One of the ways that AI can help users with a task is by developing a plan: a set of steps to solve a problem or complete a task.  Through a user study with human--AI teams, we show that AIs are poor judges of what plan is going to be more helpful to more helpful to a user trying to answer math questions or questions that require multiple steps of research (e.g., what's the tallest building in the most populous city in Germany).  
