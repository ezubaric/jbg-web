~~ Bibtex | inproceedings
~~ Title | Cold-start Active Learning through Self-Supervised Language Modeling
~~ Author | Michelle Yuan and Hsuan-Tien Lin and Jordan Boyd-Graber
~~ Booktitle | Empirical Methods in Natural Language Processing
~~ Year | 2020
~~ Location | The Cyberverse Simulacrum of Punta Cana, Dominican Republic
~~ Category | Deep Learning
~~ Category | Empirical Human Data Collection
~~ Category | Human-Computer Interaction
~~ Category | Large Language Models (or, more correctly, <A HREF="https://www.youtube.com/watch?v=u0DgoRVLTE8">Muppet Models</A>)
~~ Venue | Refereed Conference
~~ Url | docs/2020_emnlp_alps.pdf
~~ Link | Video*http://youtu.be/s470dWinVNY
~~ Link | Code*https://github.com/forest-snow/alps
~~ NumPages | 9
~~ Project | BETTER*../projects/better.html
~~ Embed | <iframe width="300" height="160" src="https://www.youtube.com/embed/s470dWinVNY" frameborder="0" allowfullscreen></iframe>
~~ Public | Labeling data is a fundamental bottleneck in machine learning, especially for NLP, due to annotation cost and time.  For medical text, obtaining labeled data is challenging because of privacy issues or shortage in expertise.  Thus, active learning can be employed to recognize the most relevant examples and then query labels from an oracle.  However, developing a strategy for selecting examples to label is non-trivial.  Active learning is difficult to use in cold-start; all examples confuse the model because it has not trained on enough data.  Fortunately, modern NLP provides an additional source of information: pre-trained language models.  In our paper, we propose an active learning strategy called ALPS to find sentences that perplex the language model.  We evaluate our approach on sentence classification datasets spanning across different domains.  Results show that ALPS is an efficient active learning strategy that is competitive with state-of-the-art approaches. 
~~ Acceptance | 25