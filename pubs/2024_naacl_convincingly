~~ Bibtex | inproceedings
~~ Title | Large Language Models Help Humans Verify Truthfulness---Except When They Are Convincingly Wrong
~~ Author | Chenglei Si and Navita Goyal and Tongshuang Wu and Chen Zhao and Shi Feng and Hal Daum\'{e} {III} and Jordan Boyd-Graber
~~ Booktitle | North American Association for Computational Linguistics
~~ Year | 2024
~~ Category | Large Language Models (or, more correctly, <A HREF="https://www.youtube.com/watch?v=u0DgoRVLTE8">Muppet Models</A>)
~~ Category | Empirical Human Data Collection
~~ Category | Fact Checking
~~ Venue | Refereed Conference
~~ Project | FACT*../projects/FACT.html
~~ Url | docs/2024_naacl_convincingly.pdf
~~ Public | Large language models can help people verify factual claims, comparing LLM-generated explanations with traditional search results in controlled human experiments. LLM explanations help users verify claims much faster than search, with similar overall accuracyâ€”but users dangerously over-trust explanations when the model is wrong, leading to worse-than-baseline performance. Presenting contrastive explanations (arguments for and against a claim) reduces over-reliance, but does not outperform simply showing retrieved evidence. Overall, the results show that fluent explanations are not a reliable substitute for primary sources in high-stakes reasoning tasks, highlighting the need for better trust calibration and evaluation of AI-assisted reasoning tools.