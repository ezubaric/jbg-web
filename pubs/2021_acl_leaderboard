~~ Bibtex | inproceedings
~~ Title |  Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?
~~ Author | Pedro Rodriguez and Joe Barrow and Alexander Hoyle and John P. Lalor and Robin Jia and Jordan Boyd-Graber
~~ Booktitle | Association for Computational Linguistics
~~ Year | 2021
~~ Category | Empirical Human Data Collection
~~ Category | Variational Inference
~~ Category | Item Response Theory
~~ Category | Question Answering
~~ Venue | Refereed Conference
~~ Project | CAREER*../projects/IIS-1652666.html
~~ Url | docs/2021_acl_leaderboard.pdf
~~ Link | Results and Code*https://leaderboard.pedro.ai/
~~ Link | Paper Read Aloud*https://youtu.be/BYzFuYzEqI8
~~ Public | When can we call an AI "intelligent"?  Just like humans, a common approach is to ask them a bunch of questions.  These questions posed to modern machine learning methods are collected in metrics called leaderboards to monitor progress, but beyond ranking approaches, this does not help us better understand our problems or our systems very well.  This paper introduces probabilistic models inspired by psychometric approaches called item response theory models (think year-end standardized tests) to better understand how computers can answer questions and whether we are asking the right questions.  This allows researchers to better compare what kinds of questions systems can answer, better compare human and machine ability, and discover problematic questions (e.g., questions that have incorrect answer keys, are vague, or "trick" those trying to answer the questions).
~~ Link | Research Talk Video*https://www.youtube.com/watch?v=akUxtt21Mlc
~~ Link | Code and Data*http://irt.pedro.ai
~~ Embed | <iframe width="300" height="160" src="https://www.youtube.com/embed/akUxtt21Mlc" frameborder="0" allowfullscreen></iframe>
~~ NumPages | 9
~~ Acceptance | 21
